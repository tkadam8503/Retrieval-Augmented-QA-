{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bb4eb3-43ac-4166-bf15-ae18ee9ddf22",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented QA (RAG)\n",
    "\n",
    "A compact RAG system you can run locally:\n",
    "\n",
    "- **Corpus ‚Üí chunks ‚Üí embeddings ‚Üí index** (FAISS or TF-IDF fallback)\n",
    "- **Retriever**: top-k passages for a query\n",
    "- **Answerer**: small greedy LLM if available, else extractive fallback\n",
    "- **Citations**: bracket refs like `[0]`, `[1]` from retrieved passages\n",
    "\n",
    "### Modes\n",
    "- `BUILD_INDEX` ‚Äî build & save a local index from the provided corpus  \n",
    "- `ASK_ONCE` ‚Äî answer one example question (or your own)  \n",
    "- `CHAT_CLI` ‚Äî small console chat loop (type `exit` to quit)\n",
    "\n",
    "This notebook is **offline-tolerant** and runs on CPU/Windows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8b233-4181-471d-9f32-58856f7f241e",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run **Install** once.\n",
    "2. Pick a **MODE** in Config.\n",
    "3. Run all cells top ‚Üí bottom.\n",
    "\n",
    "If model downloads are blocked, we automatically switch to **TF-IDF** retrieval and an **extractive** answerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a7939c-775e-4188-a1b7-7b82c857d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentence-transformers faiss-cpu transformers scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3d35f0-99f0-4732-aefa-d6add3009abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "import math\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e685c78b-acb7-4b10-97f6-3d147b842db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: ASK_ONCE | INDEX_DIR: ./rag_index | ANSWER_BACKEND: extractive\n"
     ]
    }
   ],
   "source": [
    "MODE = \"ASK_ONCE\"             # \"BUILD_INDEX\" | \"ASK_ONCE\" | \"CHAT_CLI\"\n",
    "INDEX_DIR = \"./rag_index\"\n",
    "TOP_K = 5\n",
    "MAX_NEW_TOKENS = 220\n",
    "\n",
    "# NEW: choose how answers are produced\n",
    "#   \"extractive\"  -> deterministic: quote top passages + cite them\n",
    "#   \"llm\"         -> greedy LLM; auto-falls back to extractive if gibberish\n",
    "ANSWER_BACKEND = \"extractive\"   # ‚Üê set to \"llm\" if you really want generation\n",
    "\n",
    "print(\"MODE:\", MODE, \"| INDEX_DIR:\", INDEX_DIR, \"| ANSWER_BACKEND:\", ANSWER_BACKEND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2012a0ee-b4bc-447c-b49d-049c3ea09235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small, curated corpus (offline). Edit or extend as you like.\n",
    "DOCS = [\n",
    "    {\"id\": \"econ-1\", \"title\": \"Eurozone Q2 Growth\", \"text\": \"The eurozone economy grew by 0.3% last quarter, driven by exports and services. Analysts expect moderate momentum into next quarter.\"},\n",
    "    {\"id\": \"econ-2\", \"title\": \"Central Bank Update\", \"text\": \"The central bank maintained interest rates but signaled potential cuts if inflation cools further. Markets reacted cautiously.\"},\n",
    "    {\"id\": \"tech-1\", \"title\": \"AI Startup Funding\", \"text\": \"A startup announced record funding to scale its AI research team and expand globally. Investors cited strong product-market fit.\"},\n",
    "    {\"id\": \"space-1\", \"title\": \"New Exoplanet\", \"text\": \"Scientists identified a potentially habitable exoplanet in a nearby system. Follow-up studies will probe atmosphere composition.\"},\n",
    "    {\"id\": \"sports-1\", \"title\": \"Championship Win\", \"text\": \"The national team won the championship after a dramatic final, with a late goal in extra time sealing the victory.\"},\n",
    "    {\"id\": \"health-1\", \"title\": \"Sleep & Cognition\", \"text\": \"Multiple studies correlate consistent sleep schedules with improved memory consolidation and attention across age groups.\"},\n",
    "    {\"id\": \"env-1\", \"title\": \"Urban Trees\", \"text\": \"Expanding urban tree canopy can reduce summer peak temperatures and improve air quality at the neighborhood level.\"},\n",
    "    {\"id\": \"econ-3\", \"title\": \"Labor Market\", \"text\": \"Job vacancies eased slightly while participation remained steady, suggesting a gradual rebalancing of labor demand and supply.\"},\n",
    "    {\"id\": \"tech-2\", \"title\": \"Edge Computing\", \"text\": \"Edge computing reduces latency by processing data near the source, improving reliability for real-time applications.\"},\n",
    "    {\"id\": \"health-2\", \"title\": \"Hydration\", \"text\": \"Adequate hydration supports physical performance and cognitive function; mild dehydration can impair mood and alertness.\"},\n",
    "]\n",
    "len(DOCS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fc4397-1beb-470f-b09a-71ef99f0f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text, chunk_size=80, overlap=20):\n",
    "    # simple word-based chunker\n",
    "    words = text.split()\n",
    "    chunks, i = [], 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += max(1, chunk_size - overlap)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54086205-17ee-446b-ab31-70cf60aac6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_passages(docs, chunk_size=80, overlap=20):\n",
    "    passages = []\n",
    "    for di, d in enumerate(docs):\n",
    "        for ci, ch in enumerate(chunk_text(d[\"text\"], chunk_size, overlap)):\n",
    "            passages.append({\n",
    "                \"doc_id\": d[\"id\"],\n",
    "                \"title\": d[\"title\"],\n",
    "                \"chunk_id\": f\"{d['id']}::chunk{ci}\",\n",
    "                \"text\": ch\n",
    "            })\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773979e7-2679-4258-a1df-8bf07e98ebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " {'doc_id': 'econ-1',\n",
       "  'title': 'Eurozone Q2 Growth',\n",
       "  'chunk_id': 'econ-1::chunk0',\n",
       "  'text': 'The eurozone economy grew by 0.3% last quarter, driven by exports and services. Analysts expect moderate momentum into next quarter.'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PASSAGES = build_passages(DOCS, chunk_size=80, overlap=20)\n",
    "len(PASSAGES), PASSAGES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81e3854-7994-45a4-920e-59103563aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_BACKEND = None           # \"sbert\" or \"tfidf\"\n",
    "_sbert_model = None\n",
    "_faiss_index = None\n",
    "_tfidf = None\n",
    "_tfidf_matrix = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc59219-d9ca-4264-aca5-d6b5921d8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_load_sbert():\n",
    "    global _sbert_model, EMBED_BACKEND\n",
    "    if _sbert_model is not None: return True\n",
    "    try:\n",
    "        _sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        EMBED_BACKEND = \"sbert\"\n",
    "        print(\"[embed] using SentenceTransformers all-MiniLM-L6-v2\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[embed] SBERT unavailable:\", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c92bfa-8ee1-4e36-8fed-2b77bdb906a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_index(passages):\n",
    "    \"\"\"\n",
    "    Build an index from passages.\n",
    "    If SBERT is available -> FAISS IP index with L2-normalized embeddings.\n",
    "    Else -> TF-IDF + cosine similarity matrix.\n",
    "    \"\"\"\n",
    "    global _faiss_index, _tfidf, _tfidf_matrix, EMBED_BACKEND\n",
    "    texts = [p[\"text\"] for p in passages]\n",
    "    if try_load_sbert():\n",
    "        X = _sbert_model.encode(texts, convert_to_numpy=True)\n",
    "        faiss.normalize_L2(X)\n",
    "        _faiss_index = faiss.IndexFlatIP(X.shape[1])\n",
    "        _faiss_index.add(X.astype(np.float32))\n",
    "        EMBED_BACKEND = \"sbert\"\n",
    "        print(f\"[index] FAISS built with {len(texts)} chunks.\")\n",
    "        return {\"backend\": \"sbert\", \"vectors\": X}\n",
    "    else:\n",
    "        _tfidf = TfidfVectorizer(min_df=1, max_df=1.0, ngram_range=(1,2))\n",
    "        _tfidf_matrix = _tfidf.fit_transform(texts)  # (n_chunks, n_features)\n",
    "        EMBED_BACKEND = \"tfidf\"\n",
    "        print(f\"[index] TF-IDF built with {len(texts)} chunks.\")\n",
    "        return {\"backend\": \"tfidf\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ef4a39-3f69-42a8-8d50-1e7bf323c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k=TOP_K):\n",
    "    \"\"\"\n",
    "    Return top-k (indices, scores) for the current backend.\n",
    "    \"\"\"\n",
    "    texts = [p[\"text\"] for p in PASSAGES]\n",
    "    if EMBED_BACKEND == \"sbert\" and _faiss_index is not None:\n",
    "        qv = _sbert_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(qv)\n",
    "        D, I = _faiss_index.search(qv.astype(np.float32), min(k, len(texts)))\n",
    "        return I[0].tolist(), D[0].tolist()\n",
    "    elif EMBED_BACKEND == \"tfidf\" and _tfidf is not None:\n",
    "        \n",
    "        qv = _tfidf.transform([query])          # (1, n_features)\n",
    "        sims = cosine_similarity(qv, _tfidf_matrix)[0]\n",
    "        idx = np.argsort(-sims)[:k]\n",
    "        return idx.tolist(), sims[idx].tolist()\n",
    "    else:\n",
    "        raise RuntimeError(\"Index not built. Run BUILD_INDEX mode first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c03e25d-90b3-465f-bbb0-0577e97a97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(index_dir, meta):\n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    with open(os.path.join(index_dir, \"passages.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in PASSAGES: f.write(json.dumps(p)+\"\\n\")\n",
    "    with open(os.path.join(index_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"backend\": EMBED_BACKEND}, f)\n",
    "    if EMBED_BACKEND == \"sbert\":\n",
    "        # re-embed and save since FAISS object is not trivially serializable here\n",
    "        X = _sbert_model.encode([p[\"text\"] for p in PASSAGES], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(X)\n",
    "        fa = faiss.IndexFlatIP(X.shape[1]); fa.add(X.astype(np.float32))\n",
    "        faiss.write_index(fa, os.path.join(index_dir, \"faiss.index\"))\n",
    "        np.save(os.path.join(index_dir, \"vectors.npy\"), X.astype(np.float32))\n",
    "    else:\n",
    "        joblib.dump(_tfidf, os.path.join(index_dir, \"tfidf.joblib\"))\n",
    "        # matrix can be recomputed, keeping it simple\n",
    "    print(\"[index] saved to\", index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a28654-13f9-4e63-ba5a-1036d5696cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(index_dir):\n",
    "    global PASSAGES, EMBED_BACKEND, _faiss_index, _sbert_model, _tfidf, _tfidf_matrix\n",
    "    with open(os.path.join(index_dir, \"passages.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        PASSAGES = [json.loads(l) for l in f]\n",
    "    meta = json.load(open(os.path.join(index_dir, \"meta.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    EMBED_BACKEND = meta.get(\"backend\", \"tfidf\")\n",
    "    texts = [p[\"text\"] for p in PASSAGES]\n",
    "    if EMBED_BACKEND == \"sbert\":\n",
    "        try_load_sbert()\n",
    "        _faiss_index = faiss.read_index(os.path.join(index_dir, \"faiss.index\"))\n",
    "    else:\n",
    "        _tfidf = joblib.load(os.path.join(index_dir, \"tfidf.joblib\"))\n",
    "        _tfidf_matrix = _tfidf.transform(texts)\n",
    "    print(\"[index] loaded backend:\", EMBED_BACKEND, \"| chunks:\", len(PASSAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27739770-d60b-4789-b172-59ca78f655ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_OK, PIPE, LLM_ERR = True, None, None\n",
    "\n",
    "def try_load_llm():\n",
    "    global LLM_OK, PIPE, LLM_ERR\n",
    "    if PIPE is not None or LLM_OK is False: return\n",
    "    if ANSWER_BACKEND != \"llm\":\n",
    "        LLM_OK = False\n",
    "        return\n",
    "    try:\n",
    "        for name in [\"gpt2\", \"sshleifer/tiny-gpt2\"]:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(name)\n",
    "                if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "                model = AutoModelForCausalLM.from_pretrained(name)\n",
    "                PIPE = pipeline(\"text-generation\", model=model, tokenizer=tok,\n",
    "                                do_sample=False, temperature=None, top_k=None, top_p=None)\n",
    "                print(f\"[llm] using {name} for answer synthesis\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                LLM_ERR = f\"{name}: {e}\"; continue\n",
    "        LLM_OK = False; print(\"[llm] no model; using extractive fallback.\")\n",
    "        if LLM_ERR: print(\"[llm] last error:\", LLM_ERR)\n",
    "    except Exception as e:\n",
    "        LLM_OK, LLM_ERR = False, str(e)\n",
    "        print(\"[llm] transformers unavailable; using extractive fallback.\")\n",
    "        print(\"[llm] error:\", LLM_ERR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ca628-d518-46d0-b5ec-b81165a071da",
   "metadata": {},
   "source": [
    "Extractive answerer (deterministic & cited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61b174d-edaf-4440-a142-7668387a5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SENT_SPLIT = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "def split_sentences(t: str):\n",
    "    parts = [s.strip() for s in _SENT_SPLIT.split(t) if s.strip()]\n",
    "    return parts if parts else [t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3f9a9-b302-420d-be11-ee0c7ee85c9c",
   "metadata": {},
   "source": [
    "Gibberish detector for LLM outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41699027-d203-479a-86e2-c3f299cd751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_answer(question: str, passages: list, max_sents: int = 2) -> str:\n",
    "    # Pick 1‚Äì2 key sentences from the top passages and cite them.\n",
    "    picked = []\n",
    "    for i, p in enumerate(passages[:3]):   # look at top-3 passages\n",
    "        sents = split_sentences(p)\n",
    "        if sents:\n",
    "            picked.append(f\"{sents[0]} [{i}]\")\n",
    "        if len(picked) >= max_sents: break\n",
    "    if not picked and passages:\n",
    "        picked = [passages[0][:200] + \" [‚Ä¶] [0]\"]\n",
    "    pref = \"According to the retrieved documents, \"\n",
    "    return pref + \" \".join(picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16626a8-2cba-4139-80ac-be0061f216f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_gibberish(text: str) -> bool:\n",
    "    # Heuristic: too many non-alnum chunks or weird unicode => gibberish\n",
    "    ascii_text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    if not ascii_text.strip():  # everything got stripped\n",
    "        return True\n",
    "    tokens = re.findall(r\"[A-Za-z0-9]+\", ascii_text)\n",
    "    ratio = (sum(len(t) for t in tokens) / max(1, len(ascii_text)))\n",
    "    # Low alphabetic density ‚Üí likely junk; also flag repeated nonsense\n",
    "    if ratio < 0.55:\n",
    "        return True\n",
    "    if re.search(r\"(?:\\b\\w{3,}\\b).*\\1\", ascii_text):  # simple repeated word pattern\n",
    "        return False  # repetition alone isn't enough\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9ac40-2599-4844-adac-87cc28e4e8e6",
   "metadata": {},
   "source": [
    "Prompt builder + Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d7a2963-32f4-4ffb-82fa-ee34559e7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, passages):\n",
    "    ctx = \"\\n\\n\".join([f\"[{i}] {p}\" for i, p in enumerate(passages)])\n",
    "    return f\"You are a helpful assistant. Use ONLY the context to answer and cite like [0], [1].\\n\\nContext:\\n{ctx}\\n\\nQuestion: {question}\\n\\nAnswer:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43de470-4b2a-4f2d-8910-1c011b57aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_answer(question, passages):\n",
    "    # 1) If forced extractive or no LLM, do extractive\n",
    "    try_load_llm()\n",
    "    if ANSWER_BACKEND != \"llm\" or (not LLM_OK or PIPE is None):\n",
    "        return extractive_answer(question, passages)\n",
    "\n",
    "    # 2) Try LLM; if it looks off, fall back to extractive\n",
    "    prompt = build_prompt(question, passages)\n",
    "    out = PIPE(prompt, max_new_tokens=MAX_NEW_TOKENS)[0][\"generated_text\"]\n",
    "    ans = out.split(\"Answer:\", 1)[-1].strip()\n",
    "    if looks_like_gibberish(ans):\n",
    "        return extractive_answer(question, passages)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07fb8034-273a-45e0-a0a5-b0c6f71bacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"BUILD_INDEX\":\n",
    "    meta = fit_index(PASSAGES)\n",
    "    save_index(INDEX_DIR, meta)\n",
    "    print(\"Index built & saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e916fd26-686f-4d32-92b6-52985c298577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_passages(query, k=TOP_K):\n",
    "    idxs, scores = search(query, k)\n",
    "    results = [(i, PASSAGES[i][\"text\"], PASSAGES[i][\"title\"], PASSAGES[i][\"doc_id\"], scores[j]) for j, i in enumerate(idxs)]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e8a1102-4f5e-4f57-964f-9ef877731de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query, k=TOP_K, show=True):\n",
    "    items = retrieve_passages(query, k)\n",
    "    passages = [x[1] for x in items]\n",
    "    ans = synthesize_answer(query, passages)\n",
    "    if show:\n",
    "        print(\"Q:\", query, \"\\n\")\n",
    "        print(\"Answer:\", ans, \"\\n\")\n",
    "        print(\"Sources:\")\n",
    "        for j, (i, text, title, doc_id, score) in enumerate(items):\n",
    "            print(f\"[{j}] {title} (doc={doc_id}) ‚Äî score={round(float(score),3)}\")\n",
    "    return ans, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1758d12b-7835-4c3f-86e9-2d0de2e20444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed] using SentenceTransformers all-MiniLM-L6-v2\n",
      "[index] loaded backend: sbert | chunks: 10\n",
      "Q: What happened to eurozone growth last quarter? \n",
      "\n",
      "Answer: According to the retrieved documents, The eurozone economy grew by 0.3% last quarter, driven by exports and services. [0] The central bank maintained interest rates but signaled potential cuts if inflation cools further. [1] \n",
      "\n",
      "Sources:\n",
      "[0] Eurozone Q2 Growth (doc=econ-1) ‚Äî score=0.687\n",
      "[1] Central Bank Update (doc=econ-2) ‚Äî score=0.308\n",
      "[2] Labor Market (doc=econ-3) ‚Äî score=0.229\n",
      "[3] Championship Win (doc=sports-1) ‚Äî score=0.18\n",
      "[4] AI Startup Funding (doc=tech-1) ‚Äî score=0.175\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"ASK_ONCE\":\n",
    "    # build (if not already) then answer an example\n",
    "    if not os.path.exists(INDEX_DIR):\n",
    "        meta = fit_index(PASSAGES); save_index(INDEX_DIR, meta)\n",
    "    else:\n",
    "        load_index(INDEX_DIR)\n",
    "\n",
    "    _ = answer(\"What happened to eurozone growth last quarter?\", k=TOP_K, show=True)\n",
    "\n",
    "elif MODE == \"CHAT_CLI\":\n",
    "    # tiny CLI loop; type 'exit' to quit\n",
    "    if not os.path.exists(INDEX_DIR):\n",
    "        meta = fit_index(PASSAGES); save_index(INDEX_DIR, meta)\n",
    "    else:\n",
    "        load_index(INDEX_DIR)\n",
    "    print(\"RAG chat ‚Äî type 'exit' to quit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"> \").strip()\n",
    "        except EOFError:\n",
    "            break\n",
    "        if q.lower() in {\"exit\",\"quit\"}: break\n",
    "        _ = answer(q, k=TOP_K, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a603215-8d56-4ef6-aca1-87dc287769ee",
   "metadata": {},
   "source": [
    "## üìå Notes & Next Steps\n",
    "\n",
    "- **Backends**: The notebook prefers `SentenceTransformers + FAISS`. If the model can‚Äôt download, it falls back to **TF-IDF**.  \n",
    "- **Answering**: A tiny greedy LLM is used if available; otherwise we do a simple **extractive** synthesis from top passages (still cited).  \n",
    "- **Extending**:\n",
    "  - Replace `DOCS` with your own documents (longer strings are fine).  \n",
    "  - Increase `chunk_size`/reduce `overlap` in the chunker for larger corpora.  \n",
    "  - Add metadata (dates/authors) to improve results display.  \n",
    "- **Evaluation**:\n",
    "  - Create a small list of (question, expected snippet) pairs and check that top-k retrieved passages contain the expected snippet.  \n",
    "  - Optionally measure retrieval precision@k on your own labeled dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52015330-e2b5-4e4b-9c36-99a45173ab56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
